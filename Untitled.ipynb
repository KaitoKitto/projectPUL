{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from dataset import DataSet\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\"if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cnn_kernel_size = 64\n",
    "        self.cnn_strides = 8   # when chang 10\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(self.input_size, 64, self.cnn_kernel_size, self.cnn_strides, bias=False),\n",
    "            nn.PReLU()\n",
    "            )\n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv1d(32,64,self.cnn_kernel_size,self.cnn_strides),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.gru = nn.GRU(64, hidden_size, n_layers,\n",
    "                          dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, x, len_seq, hidden=None):\n",
    "        x = x.permute(1,2,0)  # [B*N*T]\n",
    "\n",
    "        padding = self.cnn_kernel_size - x.size(2) % self.cnn_strides\n",
    "        x = F.pad(x, (0,padding))\n",
    "        x = self.cnn(x)\n",
    "\n",
    "        # padding = self.cnn_kernel_size - x.size(2) % self.cnn_strides\n",
    "        # x = F.pad(x, (0,padding))\n",
    "        # x = self.cnn_2(x)\n",
    "\n",
    "        x = x.permute(2,0,1).contiguous()  # [T*B*N]\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x,len_seq)\n",
    "        outputs, hidden = self.gru(x, hidden)\n",
    "        outputs = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # pad_packed_sequence return a tuple\n",
    "        # r_tuple[0] is the padded sequence\n",
    "        # and r_tuple[1] is a tensor contained length of sequence\n",
    "        outputs = (outputs[0][:, :, :self.hidden_size] +\n",
    "                   outputs[0][:, :, self.hidden_size:])\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, len_seq):\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1,  1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        new_atten = []\n",
    "        for i,l in enumerate(len_seq):\n",
    "            if l == timestep:\n",
    "                temp_atten = F.softmax(attn_energies[i:i+1,],dim=1)\n",
    "            else:\n",
    "                # temp_atten = torch.cat([F.softmax(attn_energies[i:i+1,:l],dim=1),Variable(torch.zeros(1,timestep-l)).cuda()],1)\n",
    "                temp_atten = torch.cat([F.softmax(attn_energies[i:i + 1, :l], dim=1), Variable(torch.zeros(1, timestep - l)).to(device)], 1)\n",
    "            new_atten.append(temp_atten)\n",
    "        new_atten = torch.cat(new_atten,0)\n",
    "        return new_atten.unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        energy = F.relu(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,\n",
    "                 n_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + output_size, hidden_size,\n",
    "                          n_layers, dropout=dropout)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, output_size)\n",
    "            )\n",
    "\n",
    "    def forward(self, input, last_hidden, encoder_outputs, len_seq):\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        # embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = input.unsqueeze(0)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs, len_seq)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        # output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio=0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward(self, src, trg, len_seq, teacher_forcing_ratio=None, is_analyse=False):\n",
    "        batch_size = trg.size(1)\n",
    "        max_len = trg.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        # outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).to(device)\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src, len_seq)\n",
    "        hidden = hidden[:self.decoder.n_layers]\n",
    "        output = Variable(trg.data[0,])  # sos\n",
    "\n",
    "        if is_analyse:\n",
    "            analyse_data = OrderedDict()\n",
    "            analyse_data['fea_after_encoder'] = encoder_output.data.cpu().numpy()\n",
    "            analyse_data['atten'] = []\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output, len_seq)\n",
    "            outputs[t] = output\n",
    "            if teacher_forcing_ratio == None:\n",
    "                teacher_forcing_ratio = self.teacher_forcing_ratio\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            # output = Variable(trg.data[t,] if is_teacher else output).cuda()\n",
    "            output = Variable(trg.data[t,] if is_teacher else output).to(device)\n",
    "            if is_analyse:\n",
    "                analyse_data['atten'].append(attn_weights.data.cpu().numpy())\n",
    "        if is_analyse:\n",
    "            analyse_data['atten'] = np.concatenate(analyse_data['atten'],axis=0)\n",
    "            return outputs, analyse_data\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RUL():\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 200\n",
    "        self.epochs = 500\n",
    "        self.lr = 1e-3\n",
    "        self.gama = 0.7\n",
    "        self.dataset = DataSet.load_dataset(name='phm_data')\n",
    "        self.train_bearings = ['Bearing1_1','Bearing1_2','Bearing2_1','Bearing2_2','Bearing3_1','Bearing3_2']\n",
    "        self.test_bearings = ['Bearing1_3','Bearing1_4','Bearing1_5','Bearing1_6','Bearing1_7',\n",
    "                                'Bearing2_3','Bearing2_4','Bearing2_5','Bearing2_6','Bearing2_7',\n",
    "                                'Bearing3_3']\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        train_data,train_label = self._preprocess('train')\n",
    "        train_iter = [[train_data[i],train_label[i]] for i in range(len(train_data))]\n",
    "        test_data,test_label = self._preprocess('test')\n",
    "        val_iter = [[test_data[i],test_label[i]] for i in range(len(test_data))]\n",
    "\n",
    "        encoder = Encoder(self.feature_size,self.hidden_size,n_layers=2,dropout=0.5)\n",
    "        decoder = Decoder(self.hidden_size,1,n_layers=2,dropout=0.5)\n",
    "        # seq2seq = Seq2Seq(encoder,decoder).cuda()\n",
    "        seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
    "        # seq2seq = torch.load('./model/newest_seq2seq')\n",
    "        seq2seq.teacher_forcing_ratio = 0.3\n",
    "        optimizer = optim.Adam(seq2seq.parameters(), lr=self.lr)\n",
    "        # optimizer = optim.RMSprop(seq2seq.parameters(), lr=self.lr)\n",
    "        # optimizer = optim.SGD(seq2seq.parameters(), lr=self.lr, momentum=0.5)\n",
    "        # optimizer = optim.LBFGS(seq2seq.parameters(),lr=self.lr)\n",
    "\n",
    "        log = OrderedDict()\n",
    "        log['train_loss'] = []\n",
    "        log['val_loss'] = []\n",
    "        log['test_loss'] = []\n",
    "        log['teacher_ratio'] = []\n",
    "        count = 0\n",
    "        count2 = 0\n",
    "        e0 = 15\n",
    "        for e in range(1, self.epochs+1):\n",
    "            train_loss = self._fit(e, seq2seq, optimizer, train_iter)\n",
    "            val_loss = self._evaluate(seq2seq, train_iter)\n",
    "            test_loss = self._evaluate(seq2seq, val_iter)\n",
    "            print(\"[Epoch:%d][train_loss:%.4e][val_loss:%.4e][test_loss:%.4e][teacher_ratio:%.4f] \"\n",
    "                % (e, train_loss, val_loss, test_loss, seq2seq.teacher_forcing_ratio))\n",
    "            log['train_loss'].append(float(train_loss))\n",
    "            log['val_loss'].append(float(val_loss))\n",
    "            log['test_loss'].append(float(test_loss))\n",
    "            log['teacher_ratio'].append(seq2seq.teacher_forcing_ratio)\n",
    "\n",
    "            '''\n",
    "            pd.DataFrame(log).to_csv('./model/log.csv',index=False)\n",
    "            '''\n",
    "            filename = './model/log.csv'\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename, 'wb') as f:\n",
    "                pd.DataFrame(log).to_csv('./model/log.csv',index=False)\n",
    "\n",
    "            if float(val_loss) == min(log['val_loss']):\n",
    "                torch.save(seq2seq, './model/seq2seq')\n",
    "            torch.save(seq2seq, './model/newest_seq2seq')\n",
    "\n",
    "            count2 += 1\n",
    "            if float(train_loss) <= float(val_loss)*0.2:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 0\n",
    "            if count >= 3 or count2 >= 50:\n",
    "                seq2seq.teacher_forcing_ratio = max(1e-5,self.gama*seq2seq.teacher_forcing_ratio)\n",
    "                count -= 1\n",
    "                count2 = 0\n",
    "\n",
    "            # optimizer.param_groups[0]['lr'] = (self.lr - (e%e0) * (self.lr-1e-7) / e0)*0.99**e\n",
    "\n",
    "    def test(self):\n",
    "        train_data,train_label = self._preprocess('train')\n",
    "        train_iter = [[train_data[i],train_label[i]] for i in range(len(train_data))]\n",
    "        test_data,test_label = self._preprocess('test')\n",
    "        val_iter = [[test_data[i],test_label[i]] for i in range(len(test_data))]\n",
    "\n",
    "        seq2seq = torch.load('./model/seq2seq')\n",
    "        self._plot_result(seq2seq, train_iter, val_iter)\n",
    "\n",
    "    def analyse(self):\n",
    "        analyse_data = OrderedDict()\n",
    "        train_data, train_data_no_norm, train_label = self._preprocess('train',is_analyse=True)\n",
    "        train_iter = [[train_data[i],train_label[i]] for i in range(len(train_data))]\n",
    "        test_data, test_data_no_norm, test_label = self._preprocess('test',is_analyse=True)\n",
    "        val_iter = [[test_data[i],test_label[i]] for i in range(len(test_data))]\n",
    "\n",
    "        analyse_data['train_data'] = train_data\n",
    "        analyse_data['train_data_no_norm'] = train_data_no_norm\n",
    "        analyse_data['train_label'] = train_label\n",
    "        analyse_data['test_data'] = test_data\n",
    "        analyse_data['test_data_no_norm'] = test_data_no_norm\n",
    "        analyse_data['test_label'] = test_label\n",
    "\n",
    "        seq2seq = torch.load('./model/seq2seq')\n",
    "        seq2seq.eval()\n",
    "\n",
    "        analyse_data['train_fea_after_encoder'] = []\n",
    "        analyse_data['train_atten'] = []\n",
    "        analyse_data['train_result'] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for [data, label] in train_iter:\n",
    "                data, label = torch.from_numpy(data.copy()), torch.from_numpy(label.copy())\n",
    "                data, label = data.type(torch.FloatTensor), label.type(torch.FloatTensor)\n",
    "                # data = Variable(data).cuda()\n",
    "                data = Variable(data).to(device)\n",
    "                # label = Variable(label).cuda()\n",
    "                label = Variable(label).to(device)\n",
    "                output, temp_analyse_data = seq2seq(data, label, teacher_forcing_ratio=0.0, is_analyse=True)\n",
    "                analyse_data['train_result'].append(output.data.cpu().numpy())\n",
    "                analyse_data['train_fea_after_encoder'].append(temp_analyse_data['fea_after_encoder'])\n",
    "                analyse_data['train_atten'].append(temp_analyse_data['atten'])\n",
    "\n",
    "        analyse_data['test_fea_after_encoder'] = []\n",
    "        analyse_data['test_atten'] = []\n",
    "        analyse_data['test_result'] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for [data, label] in val_iter:\n",
    "                data, label = torch.from_numpy(data.copy()), torch.from_numpy(label.copy())\n",
    "                data, label = data.type(torch.FloatTensor), label.type(torch.FloatTensor)\n",
    "                # data = Variable(data).cuda()\n",
    "                data = Variable(data).to(device)\n",
    "                # label = Variable(label).cuda()\n",
    "                label = Variable(label).to(device)\n",
    "                output, temp_analyse_data = seq2seq(data, label, teacher_forcing_ratio=0.0, is_analyse=True)\n",
    "                analyse_data['test_result'].append(output.data.cpu().numpy())\n",
    "                analyse_data['test_fea_after_encoder'].append(temp_analyse_data['fea_after_encoder'])\n",
    "                analyse_data['test_atten'].append(temp_analyse_data['atten'])\n",
    "\n",
    "        sio.savemat('analyse_data.mat',analyse_data)\n",
    "\n",
    "    def _custom_loss(self, pred, tru, seq_len):\n",
    "        total_loss = 0\n",
    "        for i,l in enumerate(seq_len):\n",
    "            loss = torch.mean((tru[:l,i,:] - pred[:l,i,:])**2)\n",
    "            total_loss += loss\n",
    "        return total_loss/len(seq_len)\n",
    "\n",
    "        \n",
    "    def _evaluate(self, model, val_iter):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        for [data, label] in val_iter:\n",
    "            with torch.no_grad():\n",
    "                data, label = torch.from_numpy(data.copy()), torch.from_numpy(label.copy())\n",
    "                data, label = data.type(torch.FloatTensor), label.type(torch.FloatTensor)\n",
    "                # data = Variable(data).cuda()\n",
    "                data = Variable(data).to(device)\n",
    "                # label = Variable(label).cuda()\n",
    "                label = Variable(label).to(device)\n",
    "                output = model(data, label, len_seq=[label.size(0)], teacher_forcing_ratio=0.0)\n",
    "            loss = F.mse_loss(output,label)\n",
    "            total_loss += loss.data\n",
    "        return total_loss / len(val_iter)\n",
    "\n",
    "\n",
    "    def _fit(self, e, model, optimizer, train_iter, grad_clip=10.0):\n",
    "        model.train()\n",
    "        batchbyn = 1\n",
    "        max_data_len = 0\n",
    "        max_label_len = 0\n",
    "        random.shuffle(train_iter)\n",
    "        train_data = []\n",
    "        train_label = []\n",
    "        seq_label_len = []\n",
    "        for [data, label] in train_iter:\n",
    "            for _ in range(batchbyn):\n",
    "                random_idx = random.randint(0,round(label.shape[0]*0.3))\n",
    "                train_data.append(data[random_idx*8:,]) # when chang 10\n",
    "                train_label.append(label[random_idx:,])\n",
    "                max_data_len = max(max_data_len,data.shape[0]-random_idx*8) # when chang 10\n",
    "                max_label_len = max(max_label_len,label.shape[0]-random_idx)\n",
    "                seq_label_len.append(label.shape[0]-random_idx)\n",
    "        \n",
    "        for i,data in enumerate(train_data):\n",
    "            train_data[i] = np.concatenate((data,np.zeros((max_data_len-data.shape[0],1,self.feature_size))),axis=0)\n",
    "        for i,label in enumerate(train_label):\n",
    "            train_label[i] = np.concatenate((label,-np.ones((max_label_len-label.shape[0],1,1))),axis=0)\n",
    "        train_data = np.concatenate(train_data,axis=1)\n",
    "        train_label = np.concatenate(train_label,axis=1)\n",
    "\n",
    "        sorted_len_seq_t = sorted(enumerate(seq_label_len), key=lambda x:x[1],reverse=True)\n",
    "        sorted_len_seq = [x for (_,x) in sorted_len_seq_t]\n",
    "        sorted_index = [x for (x,_) in sorted_len_seq_t]\n",
    "        train_data = train_data[:,sorted_index,:]\n",
    "        train_label = train_label[:,sorted_index,:]\n",
    "\n",
    "        train_data, train_label = torch.from_numpy(train_data), torch.from_numpy(train_label)\n",
    "        train_data, train_label = train_data.type(torch.FloatTensor), train_label.type(torch.FloatTensor)\n",
    "        # train_data, train_label = Variable(train_data).cuda(), Variable(train_label).cuda()\n",
    "        train_data, train_label = Variable(train_data).to(device), Variable(train_label).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_data, train_label, sorted_len_seq)\n",
    "        # loss = F.mse_loss(output, train_label)\n",
    "        loss = self._custom_loss(output,train_label,sorted_len_seq)\n",
    "        loss.backward()\n",
    "        # clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        return loss.data\n",
    "\n",
    "\n",
    "    def _plot_result(self, model, train_iter, val_iter):\n",
    "        model.eval()\n",
    "\n",
    "        labels = []\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for [data, label] in train_iter:\n",
    "                data, label = torch.from_numpy(data.copy()), torch.from_numpy(label.copy())\n",
    "                data, label = data.type(torch.FloatTensor), label.type(torch.FloatTensor)\n",
    "                # data = Variable(data).cuda()\n",
    "                data = Variable(data).to(device)\n",
    "                # label = Variable(label).cuda()\n",
    "                label = Variable(label).to(device)\n",
    "                output = model(data, label, teacher_forcing_ratio=0.0)\n",
    "                labels.append(label.data.cpu().numpy())\n",
    "                outputs.append(output.data.cpu().numpy())\n",
    "        labels = np.concatenate(tuple(x for x in labels), axis=0)\n",
    "        outputs = np.concatenate(tuple(x for x in outputs), axis=0)\n",
    "        labels, outputs = labels.reshape(-1,), outputs.reshape(-1,)\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(labels)\n",
    "        plt.plot(outputs)\n",
    "\n",
    "        labels = []\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for [data, label] in val_iter:\n",
    "                data, label = torch.from_numpy(data.copy()), torch.from_numpy(label.copy())\n",
    "                data, label = data.type(torch.FloatTensor), label.type(torch.FloatTensor)\n",
    "                # data = Variable(data).cuda()\n",
    "                data = Variable(data).to(device)\n",
    "                # label = Variable(label).cuda()\n",
    "                label = Variable(label).to(device)\n",
    "                output = model(data, label, teacher_forcing_ratio=0.0)\n",
    "                labels.append(label.data.cpu().numpy())\n",
    "                outputs.append(output.data.cpu().numpy())\n",
    "        labels = np.concatenate(tuple(x for x in labels), axis=0)\n",
    "        outputs = np.concatenate(tuple(x for x in outputs), axis=0)\n",
    "        labels, outputs = labels.reshape(-1,), outputs.reshape(-1,)\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(labels)\n",
    "        plt.plot(outputs)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def _preprocess(self, select, is_analyse=False):\n",
    "        if select == 'train':\n",
    "            temp_data = self.dataset.get_value('data',condition={'bearing_name':self.train_bearings})\n",
    "            temp_label = self.dataset.get_value('RUL',condition={'bearing_name':self.train_bearings})\n",
    "        elif select == 'test':\n",
    "            temp_data = self.dataset.get_value('data',condition={'bearing_name':self.test_bearings})\n",
    "            temp_label = self.dataset.get_value('RUL',condition={'bearing_name':self.test_bearings})\n",
    "        else:\n",
    "            raise ValueError('wrong selection!')\n",
    "\n",
    "        for i,x in enumerate(temp_label):\n",
    "            temp_label[i] = np.arange(temp_data[i].shape[0]) + x\n",
    "            temp_label[i] = temp_label[i][:,np.newaxis,np.newaxis]\n",
    "            temp_label[i] = temp_label[i] / np.max(temp_label[i])\n",
    "            temp_label[i] = temp_label[i][::8] # when chang 10\n",
    "        for i,x in enumerate(temp_data):\n",
    "            temp_data[i] = x[::-1,].transpose(0,2,1)\n",
    "        time_feature = [self._get_time_fea(x) for x in temp_data]\n",
    "        if is_analyse:\n",
    "            time_feature_no_norm = [self._get_time_fea(x, is_norm=False) for x in temp_data]\n",
    "            return time_feature, time_feature_no_norm, temp_label\n",
    "        else:\n",
    "            return time_feature, temp_label\n",
    "\n",
    "    def _get_time_fea(self, data, is_norm=True):\n",
    "        fea_dict = OrderedDict()\n",
    "        fea_dict['mean'] = np.mean(data,axis=2,keepdims=True)\n",
    "        fea_dict['rms'] = np.sqrt(np.mean(data**2,axis=2,keepdims=True))\n",
    "        fea_dict['kur'] = np.sum((data-fea_dict['mean'].repeat(data.shape[2],axis=2))**4,axis=2) \\\n",
    "                / (np.var(data,axis=2)**2*data.shape[2])\n",
    "        fea_dict['kur'] = fea_dict['kur'][:,:,np.newaxis]\n",
    "        fea_dict['skew'] = np.sum((data-fea_dict['mean'].repeat(data.shape[2],axis=2))**3,axis=2) \\\n",
    "                / (np.var(data,axis=2)**(3/2)*data.shape[2])\n",
    "        fea_dict['skew'] = fea_dict['skew'][:,:,np.newaxis]\n",
    "        fea_dict['p2p'] = np.max(data,axis=2,keepdims=True) - np.min(data,axis=2,keepdims=True)\n",
    "        fea_dict['var'] = np.var(data,axis=2,keepdims=True)\n",
    "        fea_dict['cre'] = np.max(abs(data),axis=2,keepdims=True) / fea_dict['rms']\n",
    "        fea_dict['imp'] = np.max(abs(data),axis=2,keepdims=True) \\\n",
    "                / np.mean(abs(data),axis=2,keepdims=True)\n",
    "        fea_dict['mar'] = np.max(abs(data),axis=2,keepdims=True) \\\n",
    "                / (np.mean((abs(data))**0.5,axis=2,keepdims=True))**2\n",
    "        fea_dict['sha'] = fea_dict['rms'] / np.mean(abs(data),axis=2,keepdims=True)\n",
    "        fea_dict['smr'] = (np.mean((abs(data))**0.5,axis=2,keepdims=True))**2\n",
    "        fea_dict['cle'] = fea_dict['p2p'] / fea_dict['smr']\n",
    "\n",
    "        fea = np.concatenate(tuple(x for x in fea_dict.values()),axis=2)\n",
    "        fea = fea.reshape(-1,fea.shape[1]*fea.shape[2])\n",
    "        self.feature_size = fea.shape[1]\n",
    "        if is_norm:\n",
    "            fea = self._normalize(fea,dim=1)\n",
    "        fea = fea[:,np.newaxis,:]\n",
    "        return fea\n",
    "    \n",
    "    def _get_fre_fea(self, data):\n",
    "        pass\n",
    "\n",
    "    def _normalize(self, data, dim=None):\n",
    "        if dim == None:\n",
    "            mmrange = 10.**np.ceil(np.log10(np.max(data) - np.min(data)))\n",
    "            r_data = (data - np.min(data)) / mmrange\n",
    "        else:\n",
    "            mmrange = 10.**np.ceil(np.log10(np.max(data,axis=dim,keepdims=True) - np.min(data,axis=dim,keepdims=True)))\n",
    "            # mmrange = np.max(data,axis=dim,keepdims=True) - np.min(data,axis=dim,keepdims=True)\n",
    "            r_data = (data - np.min(data,axis=dim,keepdims=True).repeat(data.shape[dim],axis=dim)) \\\n",
    "                / (mmrange).repeat(data.shape[dim],axis=dim)\n",
    "        return r_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset  phm_data  has been load\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    process = RUL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1][train_loss:4.1738e-01][val_loss:8.0215e-02][test_loss:7.6321e-02][teacher_ratio:0.3000] \n",
      "[Epoch:2][train_loss:7.0436e-02][val_loss:4.1159e-01][test_loss:2.5397e-01][teacher_ratio:0.3000] \n",
      "[Epoch:3][train_loss:2.7247e-01][val_loss:1.4676e-01][test_loss:7.9132e-02][teacher_ratio:0.3000] \n",
      "[Epoch:4][train_loss:1.0492e-01][val_loss:7.9893e-02][test_loss:6.6160e-02][teacher_ratio:0.3000] \n",
      "[Epoch:5][train_loss:6.8398e-02][val_loss:9.8082e-02][test_loss:1.0544e-01][teacher_ratio:0.3000] \n",
      "[Epoch:6][train_loss:1.0008e-01][val_loss:1.0583e-01][test_loss:1.1819e-01][teacher_ratio:0.3000] \n",
      "[Epoch:7][train_loss:1.1331e-01][val_loss:9.6255e-02][test_loss:1.0426e-01][teacher_ratio:0.3000] \n",
      "[Epoch:8][train_loss:1.0649e-01][val_loss:8.3348e-02][test_loss:8.0434e-02][teacher_ratio:0.3000] \n",
      "[Epoch:9][train_loss:7.9210e-02][val_loss:7.9858e-02][test_loss:6.1175e-02][teacher_ratio:0.3000] \n",
      "[Epoch:10][train_loss:6.2129e-02][val_loss:9.4058e-02][test_loss:5.6013e-02][teacher_ratio:0.3000] \n",
      "[Epoch:11][train_loss:6.5124e-02][val_loss:1.1911e-01][test_loss:6.4509e-02][teacher_ratio:0.3000] \n",
      "[Epoch:12][train_loss:7.2905e-02][val_loss:1.2790e-01][test_loss:6.8802e-02][teacher_ratio:0.3000] \n",
      "[Epoch:13][train_loss:7.8772e-02][val_loss:1.1389e-01][test_loss:6.2416e-02][teacher_ratio:0.3000] \n",
      "[Epoch:14][train_loss:5.3325e-02][val_loss:9.7769e-02][test_loss:5.6795e-02][teacher_ratio:0.3000] \n",
      "[Epoch:15][train_loss:5.2720e-02][val_loss:8.6858e-02][test_loss:5.5632e-02][teacher_ratio:0.3000] \n",
      "[Epoch:16][train_loss:4.9748e-02][val_loss:8.1823e-02][test_loss:5.7552e-02][teacher_ratio:0.3000] \n",
      "[Epoch:17][train_loss:5.5874e-02][val_loss:8.0085e-02][test_loss:5.9628e-02][teacher_ratio:0.3000] \n",
      "[Epoch:18][train_loss:5.6247e-02][val_loss:7.9755e-02][test_loss:5.9955e-02][teacher_ratio:0.3000] \n",
      "[Epoch:19][train_loss:5.7755e-02][val_loss:8.0333e-02][test_loss:5.8401e-02][teacher_ratio:0.3000] \n",
      "[Epoch:20][train_loss:6.3653e-02][val_loss:8.1572e-02][test_loss:5.6737e-02][teacher_ratio:0.3000] \n",
      "[Epoch:21][train_loss:6.0280e-02][val_loss:8.3746e-02][test_loss:5.5341e-02][teacher_ratio:0.3000] \n",
      "[Epoch:22][train_loss:6.9889e-02][val_loss:8.4868e-02][test_loss:5.4822e-02][teacher_ratio:0.3000] \n",
      "[Epoch:23][train_loss:5.1391e-02][val_loss:8.6538e-02][test_loss:5.4393e-02][teacher_ratio:0.3000] \n",
      "[Epoch:24][train_loss:5.3068e-02][val_loss:8.8146e-02][test_loss:5.4160e-02][teacher_ratio:0.3000] \n",
      "[Epoch:25][train_loss:6.6515e-02][val_loss:8.7086e-02][test_loss:5.3939e-02][teacher_ratio:0.3000] \n",
      "[Epoch:26][train_loss:5.4191e-02][val_loss:8.5560e-02][test_loss:5.3759e-02][teacher_ratio:0.3000] \n",
      "[Epoch:27][train_loss:4.7506e-02][val_loss:8.4984e-02][test_loss:5.3521e-02][teacher_ratio:0.3000] \n",
      "[Epoch:28][train_loss:5.6689e-02][val_loss:8.3437e-02][test_loss:5.3413e-02][teacher_ratio:0.3000] \n",
      "[Epoch:29][train_loss:5.0959e-02][val_loss:8.2378e-02][test_loss:5.3252e-02][teacher_ratio:0.3000] \n",
      "[Epoch:30][train_loss:5.0601e-02][val_loss:8.2443e-02][test_loss:5.2797e-02][teacher_ratio:0.3000] \n",
      "[Epoch:31][train_loss:5.8711e-02][val_loss:8.1565e-02][test_loss:5.2515e-02][teacher_ratio:0.3000] \n",
      "[Epoch:32][train_loss:4.6818e-02][val_loss:8.2141e-02][test_loss:5.1887e-02][teacher_ratio:0.3000] \n",
      "[Epoch:33][train_loss:5.2884e-02][val_loss:8.2618e-02][test_loss:5.1326e-02][teacher_ratio:0.3000] \n",
      "[Epoch:34][train_loss:4.7233e-02][val_loss:8.3720e-02][test_loss:5.0811e-02][teacher_ratio:0.3000] \n",
      "[Epoch:35][train_loss:4.6888e-02][val_loss:8.4723e-02][test_loss:5.0418e-02][teacher_ratio:0.3000] \n",
      "[Epoch:36][train_loss:4.8743e-02][val_loss:8.3963e-02][test_loss:4.9884e-02][teacher_ratio:0.3000] \n",
      "[Epoch:37][train_loss:4.0397e-02][val_loss:8.3648e-02][test_loss:4.9367e-02][teacher_ratio:0.3000] \n",
      "[Epoch:38][train_loss:5.4658e-02][val_loss:7.7073e-02][test_loss:4.8574e-02][teacher_ratio:0.3000] \n",
      "[Epoch:39][train_loss:5.1823e-02][val_loss:7.0808e-02][test_loss:5.0445e-02][teacher_ratio:0.3000] \n",
      "[Epoch:40][train_loss:4.0650e-02][val_loss:6.9724e-02][test_loss:4.8896e-02][teacher_ratio:0.3000] \n",
      "[Epoch:41][train_loss:4.7245e-02][val_loss:7.0468e-02][test_loss:4.6303e-02][teacher_ratio:0.3000] \n",
      "[Epoch:42][train_loss:3.7207e-02][val_loss:8.2012e-02][test_loss:4.7464e-02][teacher_ratio:0.3000] \n",
      "[Epoch:43][train_loss:4.6607e-02][val_loss:7.0917e-02][test_loss:4.4291e-02][teacher_ratio:0.3000] \n",
      "[Epoch:44][train_loss:3.3744e-02][val_loss:6.0100e-02][test_loss:4.4094e-02][teacher_ratio:0.3000] \n",
      "[Epoch:45][train_loss:2.9902e-02][val_loss:5.8712e-02][test_loss:4.1815e-02][teacher_ratio:0.3000] \n",
      "[Epoch:46][train_loss:3.5821e-02][val_loss:5.9048e-02][test_loss:4.2098e-02][teacher_ratio:0.3000] \n",
      "[Epoch:47][train_loss:2.1650e-02][val_loss:9.8338e-02][test_loss:7.4105e-02][teacher_ratio:0.3000] \n",
      "[Epoch:48][train_loss:4.0603e-02][val_loss:1.0219e-01][test_loss:1.1993e-01][teacher_ratio:0.3000] \n",
      "[Epoch:49][train_loss:9.2053e-02][val_loss:4.2162e-02][test_loss:3.9716e-02][teacher_ratio:0.3000] \n",
      "[Epoch:50][train_loss:2.1532e-02][val_loss:1.8308e-01][test_loss:1.4693e-01][teacher_ratio:0.3000] \n",
      "[Epoch:51][train_loss:7.6155e-02][val_loss:4.3939e-02][test_loss:4.0107e-02][teacher_ratio:0.2100] \n",
      "[Epoch:52][train_loss:2.5299e-02][val_loss:5.3794e-02][test_loss:6.2214e-02][teacher_ratio:0.2100] \n",
      "[Epoch:53][train_loss:4.1647e-02][val_loss:4.9848e-02][test_loss:5.3632e-02][teacher_ratio:0.2100] \n",
      "[Epoch:54][train_loss:3.6719e-02][val_loss:5.0440e-02][test_loss:4.0670e-02][teacher_ratio:0.2100] \n",
      "[Epoch:55][train_loss:2.6188e-02][val_loss:7.2963e-02][test_loss:4.8912e-02][teacher_ratio:0.2100] \n",
      "[Epoch:56][train_loss:3.9121e-02][val_loss:7.2995e-02][test_loss:4.8540e-02][teacher_ratio:0.2100] \n",
      "[Epoch:57][train_loss:2.3904e-02][val_loss:6.0196e-02][test_loss:4.2299e-02][teacher_ratio:0.2100] \n",
      "[Epoch:58][train_loss:2.4789e-02][val_loss:5.1642e-02][test_loss:4.1066e-02][teacher_ratio:0.2100] \n",
      "[Epoch:59][train_loss:3.7700e-02][val_loss:4.8845e-02][test_loss:4.3641e-02][teacher_ratio:0.2100] \n",
      "[Epoch:60][train_loss:2.8612e-02][val_loss:4.8222e-02][test_loss:4.3259e-02][teacher_ratio:0.2100] \n",
      "[Epoch:61][train_loss:3.0452e-02][val_loss:5.0564e-02][test_loss:4.0592e-02][teacher_ratio:0.2100] \n",
      "[Epoch:62][train_loss:2.7141e-02][val_loss:5.8985e-02][test_loss:4.3067e-02][teacher_ratio:0.2100] \n",
      "[Epoch:63][train_loss:2.4912e-02][val_loss:6.8848e-02][test_loss:4.9111e-02][teacher_ratio:0.2100] \n",
      "[Epoch:64][train_loss:3.1558e-02][val_loss:6.2541e-02][test_loss:4.6386e-02][teacher_ratio:0.2100] \n",
      "[Epoch:65][train_loss:2.3804e-02][val_loss:4.6635e-02][test_loss:4.0332e-02][teacher_ratio:0.2100] \n",
      "[Epoch:66][train_loss:2.0206e-02][val_loss:4.0991e-02][test_loss:4.1803e-02][teacher_ratio:0.2100] \n",
      "[Epoch:67][train_loss:2.5640e-02][val_loss:3.9276e-02][test_loss:4.1897e-02][teacher_ratio:0.2100] \n",
      "[Epoch:68][train_loss:2.8654e-02][val_loss:4.4658e-02][test_loss:4.1834e-02][teacher_ratio:0.2100] \n",
      "[Epoch:69][train_loss:2.4594e-02][val_loss:6.9487e-02][test_loss:5.9127e-02][teacher_ratio:0.2100] \n",
      "[Epoch:70][train_loss:2.7634e-02][val_loss:7.6255e-02][test_loss:6.5623e-02][teacher_ratio:0.2100] \n",
      "[Epoch:71][train_loss:2.2727e-02][val_loss:5.1010e-02][test_loss:4.8176e-02][teacher_ratio:0.2100] \n",
      "[Epoch:72][train_loss:2.7581e-02][val_loss:3.4840e-02][test_loss:4.1745e-02][teacher_ratio:0.2100] \n",
      "[Epoch:73][train_loss:2.2936e-02][val_loss:3.4272e-02][test_loss:4.2314e-02][teacher_ratio:0.2100] \n",
      "[Epoch:74][train_loss:2.1029e-02][val_loss:3.9197e-02][test_loss:4.2554e-02][teacher_ratio:0.2100] \n",
      "[Epoch:75][train_loss:2.2293e-02][val_loss:6.3598e-02][test_loss:5.9754e-02][teacher_ratio:0.2100] \n",
      "[Epoch:76][train_loss:1.3119e-02][val_loss:7.4875e-02][test_loss:6.9448e-02][teacher_ratio:0.2100] \n",
      "[Epoch:77][train_loss:2.0847e-02][val_loss:5.7130e-02][test_loss:5.5694e-02][teacher_ratio:0.2100] \n",
      "[Epoch:78][train_loss:1.8806e-02][val_loss:3.3165e-02][test_loss:4.1889e-02][teacher_ratio:0.2100] \n",
      "[Epoch:79][train_loss:2.4079e-02][val_loss:3.7246e-02][test_loss:5.2766e-02][teacher_ratio:0.2100] \n",
      "[Epoch:80][train_loss:3.4087e-02][val_loss:3.2403e-02][test_loss:4.4168e-02][teacher_ratio:0.2100] \n",
      "[Epoch:81][train_loss:2.4426e-02][val_loss:5.8961e-02][test_loss:5.9377e-02][teacher_ratio:0.2100] \n",
      "[Epoch:82][train_loss:2.3231e-02][val_loss:1.0112e-01][test_loss:9.4602e-02][teacher_ratio:0.2100] \n",
      "[Epoch:83][train_loss:3.0482e-02][val_loss:5.8720e-02][test_loss:5.8162e-02][teacher_ratio:0.2100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:84][train_loss:1.7539e-02][val_loss:3.4790e-02][test_loss:4.1254e-02][teacher_ratio:0.2100] \n",
      "[Epoch:85][train_loss:1.9981e-02][val_loss:3.3575e-02][test_loss:4.1753e-02][teacher_ratio:0.2100] \n",
      "[Epoch:86][train_loss:1.8478e-02][val_loss:3.6284e-02][test_loss:4.0738e-02][teacher_ratio:0.2100] \n",
      "[Epoch:87][train_loss:1.8920e-02][val_loss:4.4002e-02][test_loss:4.4475e-02][teacher_ratio:0.2100] \n",
      "[Epoch:88][train_loss:1.7782e-02][val_loss:6.1560e-02][test_loss:5.7006e-02][teacher_ratio:0.2100] \n",
      "[Epoch:89][train_loss:1.9882e-02][val_loss:5.4837e-02][test_loss:5.2141e-02][teacher_ratio:0.2100] \n",
      "[Epoch:90][train_loss:2.0447e-02][val_loss:3.7168e-02][test_loss:4.1101e-02][teacher_ratio:0.2100] \n",
      "[Epoch:91][train_loss:1.9700e-02][val_loss:3.3063e-02][test_loss:4.2395e-02][teacher_ratio:0.2100] \n",
      "[Epoch:92][train_loss:1.9502e-02][val_loss:3.2923e-02][test_loss:4.1259e-02][teacher_ratio:0.2100] \n",
      "[Epoch:93][train_loss:1.9053e-02][val_loss:4.9758e-02][test_loss:5.2081e-02][teacher_ratio:0.2100] \n",
      "[Epoch:94][train_loss:1.6585e-02][val_loss:7.3096e-02][test_loss:7.2715e-02][teacher_ratio:0.2100] \n",
      "[Epoch:95][train_loss:3.1223e-02][val_loss:3.7638e-02][test_loss:4.4862e-02][teacher_ratio:0.2100] \n",
      "[Epoch:96][train_loss:1.6430e-02][val_loss:3.3430e-02][test_loss:4.8368e-02][teacher_ratio:0.2100] \n",
      "[Epoch:97][train_loss:2.5420e-02][val_loss:3.0978e-02][test_loss:4.3657e-02][teacher_ratio:0.2100] \n",
      "[Epoch:98][train_loss:2.0799e-02][val_loss:6.2378e-02][test_loss:6.8735e-02][teacher_ratio:0.2100] \n",
      "[Epoch:99][train_loss:1.2027e-02][val_loss:1.0264e-01][test_loss:1.0586e-01][teacher_ratio:0.2100] \n",
      "[Epoch:100][train_loss:2.2422e-02][val_loss:6.1847e-02][test_loss:6.7559e-02][teacher_ratio:0.2100] \n",
      "[Epoch:101][train_loss:2.3664e-02][val_loss:3.2956e-02][test_loss:4.2632e-02][teacher_ratio:0.1470] \n",
      "[Epoch:102][train_loss:1.5474e-02][val_loss:3.2858e-02][test_loss:4.1488e-02][teacher_ratio:0.1470] \n",
      "[Epoch:103][train_loss:1.8805e-02][val_loss:4.3939e-02][test_loss:4.6734e-02][teacher_ratio:0.1470] \n",
      "[Epoch:104][train_loss:2.0033e-02][val_loss:6.3978e-02][test_loss:6.0458e-02][teacher_ratio:0.1470] \n",
      "[Epoch:105][train_loss:1.4106e-02][val_loss:5.2537e-02][test_loss:5.0352e-02][teacher_ratio:0.1470] \n",
      "[Epoch:106][train_loss:2.7398e-02][val_loss:3.4483e-02][test_loss:3.9925e-02][teacher_ratio:0.1470] \n",
      "[Epoch:107][train_loss:1.9123e-02][val_loss:3.5186e-02][test_loss:3.9697e-02][teacher_ratio:0.1470] \n",
      "[Epoch:108][train_loss:1.7677e-02][val_loss:4.7216e-02][test_loss:4.4430e-02][teacher_ratio:0.1470] \n",
      "[Epoch:109][train_loss:1.5137e-02][val_loss:6.2509e-02][test_loss:5.4866e-02][teacher_ratio:0.1470] \n",
      "[Epoch:110][train_loss:1.9334e-02][val_loss:5.0767e-02][test_loss:4.6466e-02][teacher_ratio:0.1470] \n",
      "[Epoch:111][train_loss:1.3447e-02][val_loss:4.0112e-02][test_loss:4.0392e-02][teacher_ratio:0.1470] \n",
      "[Epoch:112][train_loss:2.0730e-02][val_loss:3.5790e-02][test_loss:3.9317e-02][teacher_ratio:0.1470] \n",
      "[Epoch:113][train_loss:1.5903e-02][val_loss:4.0197e-02][test_loss:4.1211e-02][teacher_ratio:0.1470] \n",
      "[Epoch:114][train_loss:1.8732e-02][val_loss:5.6291e-02][test_loss:5.2939e-02][teacher_ratio:0.1470] \n",
      "[Epoch:115][train_loss:8.8852e-03][val_loss:7.5285e-02][test_loss:6.9341e-02][teacher_ratio:0.1470] \n",
      "[Epoch:116][train_loss:2.1889e-02][val_loss:5.3473e-02][test_loss:5.2822e-02][teacher_ratio:0.1470] \n",
      "[Epoch:117][train_loss:1.3647e-02][val_loss:4.3158e-02][test_loss:4.5449e-02][teacher_ratio:0.1470] \n",
      "[Epoch:118][train_loss:1.2801e-02][val_loss:5.0170e-02][test_loss:5.0448e-02][teacher_ratio:0.1470] \n",
      "[Epoch:119][train_loss:1.0270e-02][val_loss:4.6563e-02][test_loss:4.8514e-02][teacher_ratio:0.1470] \n",
      "[Epoch:120][train_loss:1.4291e-02][val_loss:4.1994e-02][test_loss:4.6063e-02][teacher_ratio:0.1470] \n",
      "[Epoch:121][train_loss:2.0509e-02][val_loss:2.9626e-02][test_loss:4.0660e-02][teacher_ratio:0.1470] \n",
      "[Epoch:122][train_loss:2.0566e-02][val_loss:3.9470e-02][test_loss:4.6536e-02][teacher_ratio:0.1470] \n",
      "[Epoch:123][train_loss:1.2341e-02][val_loss:9.3754e-02][test_loss:9.3234e-02][teacher_ratio:0.1470] \n",
      "[Epoch:124][train_loss:2.7990e-02][val_loss:3.7821e-02][test_loss:4.5107e-02][teacher_ratio:0.1470] \n",
      "[Epoch:125][train_loss:1.7373e-02][val_loss:3.3071e-02][test_loss:4.6643e-02][teacher_ratio:0.1470] \n",
      "[Epoch:126][train_loss:2.7580e-02][val_loss:3.9449e-02][test_loss:4.3905e-02][teacher_ratio:0.1470] \n",
      "[Epoch:127][train_loss:9.2664e-03][val_loss:1.2500e-01][test_loss:1.1019e-01][teacher_ratio:0.1470] \n",
      "[Epoch:128][train_loss:4.6663e-02][val_loss:3.7050e-02][test_loss:3.8865e-02][teacher_ratio:0.1470] \n",
      "[Epoch:129][train_loss:1.6321e-02][val_loss:3.9005e-02][test_loss:5.0864e-02][teacher_ratio:0.1470] \n",
      "[Epoch:130][train_loss:2.6217e-02][val_loss:3.5131e-02][test_loss:4.0223e-02][teacher_ratio:0.1470] \n",
      "[Epoch:131][train_loss:1.7277e-02][val_loss:6.4751e-02][test_loss:5.1790e-02][teacher_ratio:0.1470] \n",
      "[Epoch:132][train_loss:1.6070e-02][val_loss:9.0350e-02][test_loss:6.9114e-02][teacher_ratio:0.1470] \n",
      "[Epoch:133][train_loss:1.6050e-02][val_loss:6.6904e-02][test_loss:5.1990e-02][teacher_ratio:0.1470] \n",
      "[Epoch:134][train_loss:1.6134e-02][val_loss:4.2830e-02][test_loss:3.8786e-02][teacher_ratio:0.1470] \n",
      "[Epoch:135][train_loss:1.5619e-02][val_loss:3.8600e-02][test_loss:3.9213e-02][teacher_ratio:0.1470] \n",
      "[Epoch:136][train_loss:2.3139e-02][val_loss:3.8998e-02][test_loss:3.8786e-02][teacher_ratio:0.1470] \n",
      "[Epoch:137][train_loss:2.0802e-02][val_loss:4.5461e-02][test_loss:4.0404e-02][teacher_ratio:0.1470] \n",
      "[Epoch:138][train_loss:1.9381e-02][val_loss:5.7983e-02][test_loss:4.8365e-02][teacher_ratio:0.1470] \n",
      "[Epoch:139][train_loss:1.5981e-02][val_loss:5.1877e-02][test_loss:4.6105e-02][teacher_ratio:0.1470] \n",
      "[Epoch:140][train_loss:1.8989e-02][val_loss:3.3177e-02][test_loss:3.9078e-02][teacher_ratio:0.1470] \n",
      "[Epoch:141][train_loss:1.5235e-02][val_loss:3.0675e-02][test_loss:3.9726e-02][teacher_ratio:0.1470] \n",
      "[Epoch:142][train_loss:1.5727e-02][val_loss:4.9836e-02][test_loss:5.2914e-02][teacher_ratio:0.1470] \n",
      "[Epoch:143][train_loss:1.2505e-02][val_loss:8.8224e-02][test_loss:8.6645e-02][teacher_ratio:0.1470] \n",
      "[Epoch:144][train_loss:3.6316e-02][val_loss:3.1804e-02][test_loss:4.3902e-02][teacher_ratio:0.1470] \n",
      "[Epoch:145][train_loss:1.2975e-02][val_loss:2.7055e-02][test_loss:4.3527e-02][teacher_ratio:0.1470] \n",
      "[Epoch:146][train_loss:1.6664e-02][val_loss:3.7633e-02][test_loss:4.9175e-02][teacher_ratio:0.1470] \n",
      "[Epoch:147][train_loss:1.6045e-02][val_loss:7.9504e-02][test_loss:8.3408e-02][teacher_ratio:0.1470] \n",
      "[Epoch:148][train_loss:2.2118e-02][val_loss:6.8749e-02][test_loss:7.2925e-02][teacher_ratio:0.1470] \n",
      "[Epoch:149][train_loss:1.5633e-02][val_loss:2.8729e-02][test_loss:4.1045e-02][teacher_ratio:0.1470] \n",
      "[Epoch:150][train_loss:1.4002e-02][val_loss:2.8743e-02][test_loss:3.9816e-02][teacher_ratio:0.1470] \n",
      "[Epoch:151][train_loss:1.4819e-02][val_loss:5.5108e-02][test_loss:5.5541e-02][teacher_ratio:0.1029] \n",
      "[Epoch:152][train_loss:2.8447e-02][val_loss:4.1365e-02][test_loss:4.4630e-02][teacher_ratio:0.1029] \n",
      "[Epoch:153][train_loss:2.5707e-02][val_loss:2.8871e-02][test_loss:4.1616e-02][teacher_ratio:0.1029] \n",
      "[Epoch:154][train_loss:1.6466e-02][val_loss:2.8260e-02][test_loss:3.8142e-02][teacher_ratio:0.1029] \n",
      "[Epoch:155][train_loss:1.9985e-02][val_loss:4.5904e-02][test_loss:4.7148e-02][teacher_ratio:0.1029] \n",
      "[Epoch:156][train_loss:1.5125e-02][val_loss:6.2408e-02][test_loss:5.9897e-02][teacher_ratio:0.1029] \n",
      "[Epoch:157][train_loss:2.3146e-02][val_loss:2.8827e-02][test_loss:3.8265e-02][teacher_ratio:0.1029] \n",
      "[Epoch:158][train_loss:1.3467e-02][val_loss:2.9888e-02][test_loss:4.7328e-02][teacher_ratio:0.1029] \n",
      "[Epoch:159][train_loss:2.3704e-02][val_loss:3.9180e-02][test_loss:4.6856e-02][teacher_ratio:0.1029] \n",
      "[Epoch:160][train_loss:2.1884e-02][val_loss:4.7143e-02][test_loss:5.4550e-02][teacher_ratio:0.1029] \n",
      "[Epoch:161][train_loss:1.5731e-02][val_loss:1.9984e-02][test_loss:3.9178e-02][teacher_ratio:0.1029] \n",
      "[Epoch:162][train_loss:1.1161e-02][val_loss:2.2415e-02][test_loss:4.2071e-02][teacher_ratio:0.1029] \n",
      "[Epoch:163][train_loss:1.1694e-02][val_loss:9.0413e-02][test_loss:9.5553e-02][teacher_ratio:0.1029] \n",
      "[Epoch:164][train_loss:2.6711e-02][val_loss:2.4159e-02][test_loss:4.2616e-02][teacher_ratio:0.1029] \n",
      "[Epoch:165][train_loss:1.3613e-02][val_loss:5.4440e-02][test_loss:6.7095e-02][teacher_ratio:0.1029] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:166][train_loss:4.3044e-02][val_loss:9.3794e-02][test_loss:8.6637e-02][teacher_ratio:0.1029] \n",
      "[Epoch:167][train_loss:2.7199e-02][val_loss:7.4619e-02][test_loss:6.5531e-02][teacher_ratio:0.1029] \n",
      "[Epoch:168][train_loss:1.8616e-02][val_loss:3.0732e-02][test_loss:3.8945e-02][teacher_ratio:0.1029] \n",
      "[Epoch:169][train_loss:2.0343e-02][val_loss:3.5665e-02][test_loss:4.4893e-02][teacher_ratio:0.1029] \n",
      "[Epoch:170][train_loss:2.4781e-02][val_loss:4.0260e-02][test_loss:3.7352e-02][teacher_ratio:0.1029] \n",
      "[Epoch:171][train_loss:1.4107e-02][val_loss:6.9284e-02][test_loss:5.1331e-02][teacher_ratio:0.1029] \n",
      "[Epoch:172][train_loss:2.0087e-02][val_loss:7.6685e-02][test_loss:5.5201e-02][teacher_ratio:0.1029] \n",
      "[Epoch:173][train_loss:3.0560e-02][val_loss:4.5170e-02][test_loss:3.8328e-02][teacher_ratio:0.1029] \n",
      "[Epoch:174][train_loss:2.0838e-02][val_loss:4.1136e-02][test_loss:4.8297e-02][teacher_ratio:0.1029] \n",
      "[Epoch:175][train_loss:2.9865e-02][val_loss:4.3313e-02][test_loss:5.3249e-02][teacher_ratio:0.1029] \n",
      "[Epoch:176][train_loss:3.5833e-02][val_loss:4.0668e-02][test_loss:3.8401e-02][teacher_ratio:0.1029] \n",
      "[Epoch:177][train_loss:2.0225e-02][val_loss:8.1268e-02][test_loss:5.8670e-02][teacher_ratio:0.1029] \n",
      "[Epoch:178][train_loss:3.1750e-02][val_loss:9.0304e-02][test_loss:6.4855e-02][teacher_ratio:0.1029] \n",
      "[Epoch:179][train_loss:4.1900e-02][val_loss:4.8167e-02][test_loss:4.0022e-02][teacher_ratio:0.1029] \n",
      "[Epoch:180][train_loss:2.0033e-02][val_loss:3.9920e-02][test_loss:4.8647e-02][teacher_ratio:0.1029] \n",
      "[Epoch:181][train_loss:2.5754e-02][val_loss:4.6190e-02][test_loss:6.0634e-02][teacher_ratio:0.1029] \n",
      "[Epoch:182][train_loss:3.9500e-02][val_loss:3.7724e-02][test_loss:4.6389e-02][teacher_ratio:0.1029] \n",
      "[Epoch:183][train_loss:2.2244e-02][val_loss:4.5887e-02][test_loss:4.0717e-02][teacher_ratio:0.1029] \n",
      "[Epoch:184][train_loss:1.9978e-02][val_loss:7.2631e-02][test_loss:5.7225e-02][teacher_ratio:0.1029] \n",
      "[Epoch:185][train_loss:2.2887e-02][val_loss:7.2870e-02][test_loss:5.8622e-02][teacher_ratio:0.1029] \n",
      "[Epoch:186][train_loss:2.2763e-02][val_loss:4.2774e-02][test_loss:4.1563e-02][teacher_ratio:0.1029] \n",
      "[Epoch:187][train_loss:2.3500e-02][val_loss:3.2061e-02][test_loss:4.2927e-02][teacher_ratio:0.1029] \n",
      "[Epoch:188][train_loss:2.5795e-02][val_loss:3.3145e-02][test_loss:4.8235e-02][teacher_ratio:0.1029] \n",
      "[Epoch:189][train_loss:2.9306e-02][val_loss:3.1383e-02][test_loss:4.0169e-02][teacher_ratio:0.1029] \n",
      "[Epoch:190][train_loss:1.7666e-02][val_loss:5.5003e-02][test_loss:5.1884e-02][teacher_ratio:0.1029] \n",
      "[Epoch:191][train_loss:1.7351e-02][val_loss:7.3176e-02][test_loss:6.5271e-02][teacher_ratio:0.1029] \n",
      "[Epoch:192][train_loss:2.0154e-02][val_loss:5.5811e-02][test_loss:5.4009e-02][teacher_ratio:0.1029] \n",
      "[Epoch:193][train_loss:2.3377e-02][val_loss:3.4847e-02][test_loss:4.1500e-02][teacher_ratio:0.1029] \n",
      "[Epoch:194][train_loss:1.9106e-02][val_loss:2.8102e-02][test_loss:4.0455e-02][teacher_ratio:0.1029] \n",
      "[Epoch:195][train_loss:2.7515e-02][val_loss:2.7717e-02][test_loss:4.0143e-02][teacher_ratio:0.1029] \n",
      "[Epoch:196][train_loss:1.3520e-02][val_loss:3.1213e-02][test_loss:3.9958e-02][teacher_ratio:0.1029] \n",
      "[Epoch:197][train_loss:1.3003e-02][val_loss:4.6277e-02][test_loss:4.9353e-02][teacher_ratio:0.1029] \n",
      "[Epoch:198][train_loss:1.8265e-02][val_loss:5.2414e-02][test_loss:5.5056e-02][teacher_ratio:0.1029] \n",
      "[Epoch:199][train_loss:1.1937e-02][val_loss:3.8717e-02][test_loss:4.7203e-02][teacher_ratio:0.1029] \n",
      "[Epoch:200][train_loss:1.7447e-02][val_loss:2.1035e-02][test_loss:4.0355e-02][teacher_ratio:0.1029] \n",
      "[Epoch:201][train_loss:1.9745e-02][val_loss:2.2159e-02][test_loss:3.9970e-02][teacher_ratio:0.0720] \n",
      "[Epoch:202][train_loss:1.4400e-02][val_loss:4.0572e-02][test_loss:5.2420e-02][teacher_ratio:0.0720] \n",
      "[Epoch:203][train_loss:8.9960e-03][val_loss:6.5229e-02][test_loss:6.9798e-02][teacher_ratio:0.0720] \n",
      "[Epoch:204][train_loss:3.3726e-02][val_loss:1.7779e-02][test_loss:3.7557e-02][teacher_ratio:0.0720] \n",
      "[Epoch:205][train_loss:1.4210e-02][val_loss:1.9456e-02][test_loss:3.8629e-02][teacher_ratio:0.0720] \n",
      "[Epoch:206][train_loss:2.3489e-02][val_loss:5.8752e-02][test_loss:5.4877e-02][teacher_ratio:0.0720] \n",
      "[Epoch:207][train_loss:2.1602e-02][val_loss:6.0776e-02][test_loss:5.2650e-02][teacher_ratio:0.0720] \n",
      "[Epoch:208][train_loss:2.3189e-02][val_loss:2.5900e-02][test_loss:3.6271e-02][teacher_ratio:0.0720] \n",
      "[Epoch:209][train_loss:1.2715e-02][val_loss:3.7762e-02][test_loss:5.3481e-02][teacher_ratio:0.0720] \n",
      "[Epoch:210][train_loss:2.8268e-02][val_loss:2.9806e-02][test_loss:3.4109e-02][teacher_ratio:0.0720] \n",
      "[Epoch:211][train_loss:1.4594e-02][val_loss:6.7526e-02][test_loss:5.0747e-02][teacher_ratio:0.0720] \n",
      "[Epoch:212][train_loss:3.1323e-02][val_loss:5.1321e-02][test_loss:4.1132e-02][teacher_ratio:0.0720] \n",
      "[Epoch:213][train_loss:1.6174e-02][val_loss:2.9578e-02][test_loss:3.4506e-02][teacher_ratio:0.0720] \n",
      "[Epoch:214][train_loss:1.9097e-02][val_loss:2.8951e-02][test_loss:4.1121e-02][teacher_ratio:0.0720] \n",
      "[Epoch:215][train_loss:2.1450e-02][val_loss:2.6044e-02][test_loss:3.3807e-02][teacher_ratio:0.0720] \n",
      "[Epoch:216][train_loss:1.8718e-02][val_loss:4.2521e-02][test_loss:3.9649e-02][teacher_ratio:0.0720] \n",
      "[Epoch:217][train_loss:9.8334e-03][val_loss:5.7340e-02][test_loss:5.0357e-02][teacher_ratio:0.0720] \n",
      "[Epoch:218][train_loss:1.8407e-02][val_loss:1.9214e-02][test_loss:3.2786e-02][teacher_ratio:0.0720] \n",
      "[Epoch:219][train_loss:7.0124e-03][val_loss:2.2720e-02][test_loss:4.2209e-02][teacher_ratio:0.0720] \n",
      "[Epoch:220][train_loss:1.9615e-02][val_loss:2.3927e-02][test_loss:3.8946e-02][teacher_ratio:0.0720] \n",
      "[Epoch:221][train_loss:5.8318e-03][val_loss:5.0364e-02][test_loss:5.7109e-02][teacher_ratio:0.0720] \n",
      "[Epoch:222][train_loss:2.0476e-02][val_loss:8.5758e-03][test_loss:3.5754e-02][teacher_ratio:0.0720] \n",
      "[Epoch:223][train_loss:5.0499e-03][val_loss:2.3516e-01][test_loss:6.0258e-02][teacher_ratio:0.0720] \n",
      "[Epoch:224][train_loss:5.3771e-02][val_loss:1.0417e-01][test_loss:8.5589e-02][teacher_ratio:0.0720] \n",
      "[Epoch:225][train_loss:5.6521e-02][val_loss:6.8854e-02][test_loss:5.3397e-02][teacher_ratio:0.0720] \n",
      "[Epoch:226][train_loss:3.0538e-02][val_loss:3.2130e-02][test_loss:3.7193e-02][teacher_ratio:0.0720] \n",
      "[Epoch:227][train_loss:1.8125e-02][val_loss:4.1012e-02][test_loss:5.5132e-02][teacher_ratio:0.0720] \n",
      "[Epoch:228][train_loss:3.6525e-02][val_loss:4.3340e-02][test_loss:5.5370e-02][teacher_ratio:0.0720] \n",
      "[Epoch:229][train_loss:3.6063e-02][val_loss:4.0818e-02][test_loss:4.4095e-02][teacher_ratio:0.0720] \n",
      "[Epoch:230][train_loss:2.5397e-02][val_loss:4.7951e-02][test_loss:3.7574e-02][teacher_ratio:0.0720] \n",
      "[Epoch:231][train_loss:1.8408e-02][val_loss:6.8990e-02][test_loss:4.4533e-02][teacher_ratio:0.0720] \n",
      "[Epoch:232][train_loss:3.6651e-02][val_loss:7.2619e-02][test_loss:4.6410e-02][teacher_ratio:0.0720] \n",
      "[Epoch:233][train_loss:2.4104e-02][val_loss:6.3817e-02][test_loss:4.2385e-02][teacher_ratio:0.0720] \n",
      "[Epoch:234][train_loss:4.1828e-02][val_loss:4.7531e-02][test_loss:3.8497e-02][teacher_ratio:0.0720] \n",
      "[Epoch:235][train_loss:2.6916e-02][val_loss:4.2803e-02][test_loss:4.4371e-02][teacher_ratio:0.0720] \n",
      "[Epoch:236][train_loss:3.2724e-02][val_loss:4.3334e-02][test_loss:4.9636e-02][teacher_ratio:0.0720] \n",
      "[Epoch:237][train_loss:3.5051e-02][val_loss:4.1514e-02][test_loss:4.5798e-02][teacher_ratio:0.0720] \n",
      "[Epoch:238][train_loss:3.1636e-02][val_loss:4.0841e-02][test_loss:3.9990e-02][teacher_ratio:0.0720] \n",
      "[Epoch:239][train_loss:2.6902e-02][val_loss:4.7569e-02][test_loss:3.8227e-02][teacher_ratio:0.0720] \n",
      "[Epoch:240][train_loss:3.1876e-02][val_loss:5.4968e-02][test_loss:4.1040e-02][teacher_ratio:0.0720] \n",
      "[Epoch:241][train_loss:3.4535e-02][val_loss:4.9163e-02][test_loss:3.9286e-02][teacher_ratio:0.0720] \n",
      "[Epoch:242][train_loss:3.1117e-02][val_loss:3.7362e-02][test_loss:3.6851e-02][teacher_ratio:0.0720] \n",
      "[Epoch:243][train_loss:1.4917e-02][val_loss:3.1234e-02][test_loss:3.8237e-02][teacher_ratio:0.0720] \n",
      "[Epoch:244][train_loss:2.7166e-02][val_loss:2.8024e-02][test_loss:3.9086e-02][teacher_ratio:0.0720] \n",
      "[Epoch:245][train_loss:1.6883e-02][val_loss:2.5353e-02][test_loss:3.7310e-02][teacher_ratio:0.0720] \n",
      "[Epoch:246][train_loss:2.0580e-02][val_loss:2.5468e-02][test_loss:3.5270e-02][teacher_ratio:0.0720] \n",
      "[Epoch:247][train_loss:1.8060e-02][val_loss:2.6263e-02][test_loss:3.5519e-02][teacher_ratio:0.0720] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:248][train_loss:1.5220e-02][val_loss:2.5164e-02][test_loss:3.5939e-02][teacher_ratio:0.0720] \n",
      "[Epoch:249][train_loss:1.4350e-02][val_loss:2.0372e-02][test_loss:3.4274e-02][teacher_ratio:0.0720] \n",
      "[Epoch:250][train_loss:8.9747e-03][val_loss:1.9407e-01][test_loss:3.7596e-02][teacher_ratio:0.0720] \n",
      "[Epoch:251][train_loss:1.4640e-02][val_loss:2.2449e-01][test_loss:2.7365e-02][teacher_ratio:0.0504] \n",
      "[Epoch:252][train_loss:6.0923e-03][val_loss:5.2110e-02][test_loss:4.1315e-02][teacher_ratio:0.0504] \n",
      "[Epoch:253][train_loss:2.0828e-02][val_loss:1.1635e-01][test_loss:2.3404e-02][teacher_ratio:0.0504] \n",
      "[Epoch:254][train_loss:6.6428e-03][val_loss:2.0637e-01][test_loss:8.0208e-02][teacher_ratio:0.0353] \n",
      "[Epoch:255][train_loss:3.4420e-02][val_loss:3.6596e-02][test_loss:2.9338e-02][teacher_ratio:0.0247] \n",
      "[Epoch:256][train_loss:1.6774e-02][val_loss:4.4204e-02][test_loss:3.2263e-02][teacher_ratio:0.0247] \n",
      "[Epoch:257][train_loss:3.2941e-02][val_loss:2.2601e-02][test_loss:3.0215e-02][teacher_ratio:0.0247] \n",
      "[Epoch:258][train_loss:1.4089e-02][val_loss:2.6330e-02][test_loss:4.1251e-02][teacher_ratio:0.0247] \n",
      "[Epoch:259][train_loss:2.4257e-02][val_loss:2.6444e-02][test_loss:3.6296e-02][teacher_ratio:0.0247] \n",
      "[Epoch:260][train_loss:1.8916e-02][val_loss:3.1714e-02][test_loss:3.1130e-02][teacher_ratio:0.0247] \n",
      "[Epoch:261][train_loss:1.3945e-02][val_loss:4.5955e-02][test_loss:3.5628e-02][teacher_ratio:0.0247] \n",
      "[Epoch:262][train_loss:2.5267e-02][val_loss:5.1997e-02][test_loss:3.8408e-02][teacher_ratio:0.0247] \n",
      "[Epoch:263][train_loss:2.6450e-02][val_loss:3.7021e-02][test_loss:3.2122e-02][teacher_ratio:0.0247] \n",
      "[Epoch:264][train_loss:1.9055e-02][val_loss:2.8166e-02][test_loss:3.3158e-02][teacher_ratio:0.0247] \n",
      "[Epoch:265][train_loss:1.7841e-02][val_loss:2.6948e-02][test_loss:3.5782e-02][teacher_ratio:0.0247] \n",
      "[Epoch:266][train_loss:2.1123e-02][val_loss:2.5911e-02][test_loss:3.2662e-02][teacher_ratio:0.0247] \n",
      "[Epoch:267][train_loss:1.7457e-02][val_loss:2.8349e-02][test_loss:2.9166e-02][teacher_ratio:0.0247] \n",
      "[Epoch:268][train_loss:1.6145e-02][val_loss:3.3869e-02][test_loss:2.9094e-02][teacher_ratio:0.0247] \n",
      "[Epoch:269][train_loss:1.8542e-02][val_loss:2.6916e-02][test_loss:2.7159e-02][teacher_ratio:0.0247] \n",
      "[Epoch:270][train_loss:1.3317e-02][val_loss:2.1443e-02][test_loss:2.5799e-02][teacher_ratio:0.0247] \n",
      "[Epoch:271][train_loss:1.2402e-02][val_loss:1.3996e-02][test_loss:2.7005e-02][teacher_ratio:0.0247] \n",
      "[Epoch:272][train_loss:9.4671e-03][val_loss:1.1646e-02][test_loss:2.9768e-02][teacher_ratio:0.0247] \n",
      "[Epoch:273][train_loss:9.3751e-03][val_loss:1.0237e-02][test_loss:2.3916e-02][teacher_ratio:0.0247] \n",
      "[Epoch:274][train_loss:8.5630e-03][val_loss:3.1866e-02][test_loss:2.1535e-02][teacher_ratio:0.0247] \n",
      "[Epoch:275][train_loss:1.6742e-02][val_loss:2.0664e-01][test_loss:1.8143e-02][teacher_ratio:0.0247] \n",
      "[Epoch:276][train_loss:6.1734e-03][val_loss:2.8626e-01][test_loss:3.0609e-01][teacher_ratio:0.0247] \n",
      "[Epoch:277][train_loss:2.0153e-01][val_loss:3.5037e-02][test_loss:3.2300e-02][teacher_ratio:0.0247] \n",
      "[Epoch:278][train_loss:1.1141e-02][val_loss:5.1717e-02][test_loss:4.1609e-02][teacher_ratio:0.0247] \n",
      "[Epoch:279][train_loss:1.4550e-02][val_loss:4.5962e-02][test_loss:3.7100e-02][teacher_ratio:0.0247] \n",
      "[Epoch:280][train_loss:1.9682e-02][val_loss:3.5489e-02][test_loss:3.3802e-02][teacher_ratio:0.0247] \n",
      "[Epoch:281][train_loss:2.5653e-02][val_loss:3.5924e-02][test_loss:3.9026e-02][teacher_ratio:0.0247] \n",
      "[Epoch:282][train_loss:3.3475e-02][val_loss:3.8399e-02][test_loss:4.2482e-02][teacher_ratio:0.0247] \n",
      "[Epoch:283][train_loss:3.5368e-02][val_loss:3.9206e-02][test_loss:3.9097e-02][teacher_ratio:0.0247] \n",
      "[Epoch:284][train_loss:3.2409e-02][val_loss:4.3545e-02][test_loss:3.5715e-02][teacher_ratio:0.0247] \n",
      "[Epoch:285][train_loss:2.9479e-02][val_loss:5.3506e-02][test_loss:3.7765e-02][teacher_ratio:0.0247] \n",
      "[Epoch:286][train_loss:3.4731e-02][val_loss:5.5168e-02][test_loss:3.8462e-02][teacher_ratio:0.0247] \n",
      "[Epoch:287][train_loss:2.2722e-02][val_loss:5.2882e-02][test_loss:3.7689e-02][teacher_ratio:0.0247] \n",
      "[Epoch:288][train_loss:2.7762e-02][val_loss:4.7777e-02][test_loss:3.6168e-02][teacher_ratio:0.0247] \n",
      "[Epoch:289][train_loss:2.1949e-02][val_loss:4.4989e-02][test_loss:3.5616e-02][teacher_ratio:0.0247] \n",
      "[Epoch:290][train_loss:2.3969e-02][val_loss:4.0854e-02][test_loss:3.5355e-02][teacher_ratio:0.0247] \n",
      "[Epoch:291][train_loss:2.7898e-02][val_loss:3.7011e-02][test_loss:3.6179e-02][teacher_ratio:0.0247] \n",
      "[Epoch:292][train_loss:2.5661e-02][val_loss:3.6191e-02][test_loss:3.5746e-02][teacher_ratio:0.0247] \n",
      "[Epoch:293][train_loss:2.6756e-02][val_loss:3.5928e-02][test_loss:3.5241e-02][teacher_ratio:0.0247] \n",
      "[Epoch:294][train_loss:2.6829e-02][val_loss:3.7828e-02][test_loss:3.5098e-02][teacher_ratio:0.0247] \n",
      "[Epoch:295][train_loss:2.3755e-02][val_loss:3.7830e-02][test_loss:3.5314e-02][teacher_ratio:0.0247] \n",
      "[Epoch:296][train_loss:1.6355e-02][val_loss:4.4792e-02][test_loss:3.7810e-02][teacher_ratio:0.0247] \n",
      "[Epoch:297][train_loss:2.2577e-02][val_loss:4.2914e-02][test_loss:3.7285e-02][teacher_ratio:0.0247] \n",
      "[Epoch:298][train_loss:2.0294e-02][val_loss:3.6151e-02][test_loss:3.4967e-02][teacher_ratio:0.0247] \n",
      "[Epoch:299][train_loss:1.8373e-02][val_loss:3.4550e-02][test_loss:3.3721e-02][teacher_ratio:0.0247] \n",
      "[Epoch:300][train_loss:1.8594e-02][val_loss:2.6867e-02][test_loss:3.2635e-02][teacher_ratio:0.0247] \n",
      "[Epoch:301][train_loss:1.7009e-02][val_loss:2.4958e-02][test_loss:3.1941e-02][teacher_ratio:0.0247] \n",
      "[Epoch:302][train_loss:1.3805e-02][val_loss:2.3695e-02][test_loss:3.1745e-02][teacher_ratio:0.0247] \n",
      "[Epoch:303][train_loss:1.4324e-02][val_loss:3.0701e-02][test_loss:2.8210e-02][teacher_ratio:0.0247] \n",
      "[Epoch:304][train_loss:1.5668e-02][val_loss:2.2515e-02][test_loss:3.1773e-02][teacher_ratio:0.0247] \n",
      "[Epoch:305][train_loss:8.9748e-03][val_loss:2.0370e-02][test_loss:3.2941e-02][teacher_ratio:0.0173] \n",
      "[Epoch:306][train_loss:1.3994e-02][val_loss:1.7633e-02][test_loss:3.4419e-02][teacher_ratio:0.0173] \n",
      "[Epoch:307][train_loss:1.0247e-02][val_loss:1.8683e-02][test_loss:3.1667e-02][teacher_ratio:0.0173] \n",
      "[Epoch:308][train_loss:1.2978e-02][val_loss:1.2735e-02][test_loss:5.2924e-02][teacher_ratio:0.0173] \n",
      "[Epoch:309][train_loss:1.6127e-02][val_loss:1.3326e-01][test_loss:4.6522e-02][teacher_ratio:0.0173] \n",
      "[Epoch:310][train_loss:7.9263e-02][val_loss:3.9685e-02][test_loss:1.0350e-01][teacher_ratio:0.0173] \n",
      "[Epoch:311][train_loss:5.1809e-02][val_loss:1.1728e-02][test_loss:4.8920e-02][teacher_ratio:0.0173] \n",
      "[Epoch:312][train_loss:2.2555e-02][val_loss:3.8411e-02][test_loss:4.0261e-02][teacher_ratio:0.0173] \n",
      "[Epoch:313][train_loss:3.0642e-02][val_loss:4.8610e-02][test_loss:4.5225e-02][teacher_ratio:0.0173] \n",
      "[Epoch:314][train_loss:1.6509e-02][val_loss:3.4922e-02][test_loss:3.9005e-02][teacher_ratio:0.0173] \n",
      "[Epoch:315][train_loss:1.9612e-02][val_loss:2.2953e-02][test_loss:3.6715e-02][teacher_ratio:0.0173] \n",
      "[Epoch:316][train_loss:1.6034e-02][val_loss:2.1226e-02][test_loss:4.1316e-02][teacher_ratio:0.0173] \n",
      "[Epoch:317][train_loss:3.2627e-02][val_loss:2.2386e-02][test_loss:3.6802e-02][teacher_ratio:0.0173] \n",
      "[Epoch:318][train_loss:2.6398e-02][val_loss:2.8423e-02][test_loss:3.4107e-02][teacher_ratio:0.0173] \n",
      "[Epoch:319][train_loss:1.0300e-02][val_loss:4.0572e-02][test_loss:3.6331e-02][teacher_ratio:0.0173] \n",
      "[Epoch:320][train_loss:1.7891e-02][val_loss:5.1903e-02][test_loss:3.9574e-02][teacher_ratio:0.0173] \n",
      "[Epoch:321][train_loss:2.3513e-02][val_loss:4.0948e-02][test_loss:3.3792e-02][teacher_ratio:0.0173] \n",
      "[Epoch:322][train_loss:1.6478e-02][val_loss:2.9572e-02][test_loss:3.0549e-02][teacher_ratio:0.0173] \n",
      "[Epoch:323][train_loss:1.1569e-02][val_loss:2.4035e-02][test_loss:3.1804e-02][teacher_ratio:0.0173] \n",
      "[Epoch:324][train_loss:1.4180e-02][val_loss:2.3318e-02][test_loss:3.2355e-02][teacher_ratio:0.0173] \n",
      "[Epoch:325][train_loss:1.5707e-02][val_loss:2.6661e-02][test_loss:3.0035e-02][teacher_ratio:0.0173] \n",
      "[Epoch:326][train_loss:1.2095e-02][val_loss:3.9540e-02][test_loss:3.0153e-02][teacher_ratio:0.0173] \n",
      "[Epoch:327][train_loss:2.0276e-02][val_loss:4.5121e-02][test_loss:3.1295e-02][teacher_ratio:0.0173] \n",
      "[Epoch:328][train_loss:1.4846e-02][val_loss:3.9991e-02][test_loss:2.9695e-02][teacher_ratio:0.0173] \n",
      "[Epoch:329][train_loss:1.2835e-02][val_loss:3.2292e-02][test_loss:2.8316e-02][teacher_ratio:0.0173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:330][train_loss:1.7728e-02][val_loss:2.1326e-02][test_loss:3.1202e-02][teacher_ratio:0.0173] \n",
      "[Epoch:331][train_loss:1.7488e-02][val_loss:1.8674e-02][test_loss:3.9074e-02][teacher_ratio:0.0173] \n",
      "[Epoch:332][train_loss:1.6890e-02][val_loss:1.8006e-02][test_loss:4.0004e-02][teacher_ratio:0.0173] \n",
      "[Epoch:333][train_loss:1.8600e-02][val_loss:1.7645e-02][test_loss:3.3488e-02][teacher_ratio:0.0173] \n",
      "[Epoch:334][train_loss:1.5818e-02][val_loss:2.2965e-02][test_loss:2.9380e-02][teacher_ratio:0.0173] \n",
      "[Epoch:335][train_loss:1.4967e-02][val_loss:4.0307e-02][test_loss:3.1817e-02][teacher_ratio:0.0173] \n",
      "[Epoch:336][train_loss:1.9326e-02][val_loss:4.3859e-02][test_loss:3.2635e-02][teacher_ratio:0.0173] \n",
      "[Epoch:337][train_loss:1.0359e-02][val_loss:3.5997e-02][test_loss:2.9764e-02][teacher_ratio:0.0173] \n",
      "[Epoch:338][train_loss:7.2204e-03][val_loss:2.3858e-02][test_loss:2.7488e-02][teacher_ratio:0.0173] \n",
      "[Epoch:339][train_loss:9.9886e-03][val_loss:1.4659e-02][test_loss:3.0606e-02][teacher_ratio:0.0173] \n",
      "[Epoch:340][train_loss:1.0037e-02][val_loss:1.2584e-02][test_loss:3.6289e-02][teacher_ratio:0.0173] \n",
      "[Epoch:341][train_loss:1.4453e-02][val_loss:1.2852e-02][test_loss:3.1809e-02][teacher_ratio:0.0173] \n",
      "[Epoch:342][train_loss:1.4925e-02][val_loss:3.0952e-02][test_loss:2.6390e-02][teacher_ratio:0.0173] \n",
      "[Epoch:343][train_loss:1.0745e-02][val_loss:4.3760e-02][test_loss:2.9222e-02][teacher_ratio:0.0173] \n",
      "[Epoch:344][train_loss:2.1074e-02][val_loss:2.0897e-02][test_loss:2.5216e-02][teacher_ratio:0.0173] \n",
      "[Epoch:345][train_loss:1.2845e-02][val_loss:1.3284e-02][test_loss:4.4083e-02][teacher_ratio:0.0173] \n",
      "[Epoch:346][train_loss:1.8034e-02][val_loss:1.2645e-02][test_loss:4.2723e-02][teacher_ratio:0.0173] \n",
      "[Epoch:347][train_loss:1.7798e-02][val_loss:1.9632e-02][test_loss:2.5823e-02][teacher_ratio:0.0173] \n",
      "[Epoch:348][train_loss:8.3580e-03][val_loss:3.7396e-02][test_loss:3.0148e-02][teacher_ratio:0.0173] \n",
      "[Epoch:349][train_loss:2.0612e-02][val_loss:2.6566e-02][test_loss:2.8165e-02][teacher_ratio:0.0173] \n",
      "[Epoch:350][train_loss:9.3529e-03][val_loss:1.3689e-02][test_loss:2.7969e-02][teacher_ratio:0.0173] \n",
      "[Epoch:351][train_loss:1.0697e-02][val_loss:1.2007e-02][test_loss:2.8803e-02][teacher_ratio:0.0173] \n",
      "[Epoch:352][train_loss:1.0157e-02][val_loss:1.5779e-02][test_loss:2.6483e-02][teacher_ratio:0.0173] \n",
      "[Epoch:353][train_loss:1.0994e-02][val_loss:2.8548e-02][test_loss:2.7406e-02][teacher_ratio:0.0173] \n",
      "[Epoch:354][train_loss:1.3118e-02][val_loss:2.8310e-02][test_loss:2.6858e-02][teacher_ratio:0.0173] \n",
      "[Epoch:355][train_loss:1.1514e-02][val_loss:1.5955e-02][test_loss:2.5504e-02][teacher_ratio:0.0121] \n",
      "[Epoch:356][train_loss:8.8005e-03][val_loss:1.6143e-02][test_loss:2.5044e-02][teacher_ratio:0.0121] \n",
      "[Epoch:357][train_loss:1.1464e-02][val_loss:2.1183e-02][test_loss:2.3941e-02][teacher_ratio:0.0121] \n",
      "[Epoch:358][train_loss:8.0808e-03][val_loss:2.9472e-02][test_loss:2.4840e-02][teacher_ratio:0.0121] \n",
      "[Epoch:359][train_loss:6.3220e-03][val_loss:3.0768e-02][test_loss:2.5902e-02][teacher_ratio:0.0121] \n",
      "[Epoch:360][train_loss:1.3271e-02][val_loss:2.1324e-02][test_loss:2.6080e-02][teacher_ratio:0.0121] \n",
      "[Epoch:361][train_loss:6.5349e-03][val_loss:1.5422e-02][test_loss:2.8425e-02][teacher_ratio:0.0121] \n",
      "[Epoch:362][train_loss:9.4207e-03][val_loss:1.2718e-02][test_loss:3.0799e-02][teacher_ratio:0.0121] \n",
      "[Epoch:363][train_loss:1.2602e-02][val_loss:1.6634e-02][test_loss:3.0925e-02][teacher_ratio:0.0121] \n",
      "[Epoch:364][train_loss:1.0969e-02][val_loss:2.5877e-02][test_loss:3.3289e-02][teacher_ratio:0.0121] \n",
      "[Epoch:365][train_loss:1.3212e-02][val_loss:2.6059e-02][test_loss:3.1611e-02][teacher_ratio:0.0121] \n",
      "[Epoch:366][train_loss:8.3848e-03][val_loss:2.2883e-02][test_loss:2.8074e-02][teacher_ratio:0.0121] \n",
      "[Epoch:367][train_loss:1.1787e-02][val_loss:1.4104e-02][test_loss:2.5386e-02][teacher_ratio:0.0121] \n",
      "[Epoch:368][train_loss:9.6189e-03][val_loss:1.2872e-02][test_loss:2.8098e-02][teacher_ratio:0.0121] \n",
      "[Epoch:369][train_loss:1.5518e-02][val_loss:1.4747e-02][test_loss:2.5499e-02][teacher_ratio:0.0121] \n",
      "[Epoch:370][train_loss:8.8557e-03][val_loss:3.5290e-02][test_loss:2.2735e-02][teacher_ratio:0.0121] \n",
      "[Epoch:371][train_loss:8.0484e-03][val_loss:5.5118e-02][test_loss:2.8335e-02][teacher_ratio:0.0121] \n",
      "[Epoch:372][train_loss:2.1983e-02][val_loss:2.3831e-02][test_loss:2.2826e-02][teacher_ratio:0.0121] \n",
      "[Epoch:373][train_loss:1.2931e-02][val_loss:9.9551e-03][test_loss:3.4929e-02][teacher_ratio:0.0121] \n",
      "[Epoch:374][train_loss:1.3597e-02][val_loss:9.9167e-03][test_loss:3.8239e-02][teacher_ratio:0.0121] \n",
      "[Epoch:375][train_loss:1.5425e-02][val_loss:1.1431e-02][test_loss:3.1488e-02][teacher_ratio:0.0121] \n",
      "[Epoch:376][train_loss:7.1564e-03][val_loss:2.0004e-02][test_loss:3.3930e-02][teacher_ratio:0.0121] \n",
      "[Epoch:377][train_loss:1.2159e-02][val_loss:3.0199e-02][test_loss:3.8491e-02][teacher_ratio:0.0121] \n",
      "[Epoch:378][train_loss:7.5401e-03][val_loss:2.9589e-02][test_loss:3.8032e-02][teacher_ratio:0.0121] \n",
      "[Epoch:379][train_loss:9.4831e-03][val_loss:2.0320e-02][test_loss:3.3530e-02][teacher_ratio:0.0121] \n",
      "[Epoch:380][train_loss:9.9234e-03][val_loss:1.0003e-02][test_loss:3.2648e-02][teacher_ratio:0.0121] \n",
      "[Epoch:381][train_loss:1.2353e-02][val_loss:9.1058e-03][test_loss:3.2165e-02][teacher_ratio:0.0121] \n",
      "[Epoch:382][train_loss:1.7034e-02][val_loss:1.8071e-02][test_loss:2.6589e-02][teacher_ratio:0.0121] \n",
      "[Epoch:383][train_loss:6.0065e-03][val_loss:3.6525e-02][test_loss:2.8408e-02][teacher_ratio:0.0121] \n",
      "[Epoch:384][train_loss:2.4368e-02][val_loss:2.2307e-02][test_loss:2.4137e-02][teacher_ratio:0.0121] \n",
      "[Epoch:385][train_loss:7.0313e-03][val_loss:1.4285e-02][test_loss:2.5421e-02][teacher_ratio:0.0121] \n",
      "[Epoch:386][train_loss:7.6203e-03][val_loss:1.2312e-02][test_loss:2.9962e-02][teacher_ratio:0.0121] \n",
      "[Epoch:387][train_loss:9.4115e-03][val_loss:1.4291e-02][test_loss:2.6715e-02][teacher_ratio:0.0121] \n",
      "[Epoch:388][train_loss:1.0888e-02][val_loss:3.3183e-02][test_loss:2.3464e-02][teacher_ratio:0.0121] \n",
      "[Epoch:389][train_loss:1.0629e-02][val_loss:4.8186e-02][test_loss:2.7545e-02][teacher_ratio:0.0121] \n",
      "[Epoch:390][train_loss:2.1066e-02][val_loss:2.0113e-02][test_loss:2.3689e-02][teacher_ratio:0.0121] \n",
      "[Epoch:391][train_loss:7.4891e-03][val_loss:9.3713e-03][test_loss:3.4252e-02][teacher_ratio:0.0121] \n",
      "[Epoch:392][train_loss:1.0454e-02][val_loss:8.6703e-03][test_loss:3.6394e-02][teacher_ratio:0.0121] \n",
      "[Epoch:393][train_loss:1.9139e-02][val_loss:1.7013e-02][test_loss:2.7524e-02][teacher_ratio:0.0121] \n",
      "[Epoch:394][train_loss:7.5215e-03][val_loss:3.4282e-02][test_loss:3.1322e-02][teacher_ratio:0.0121] \n",
      "[Epoch:395][train_loss:1.0551e-02][val_loss:4.5315e-02][test_loss:3.5090e-02][teacher_ratio:0.0121] \n",
      "[Epoch:396][train_loss:1.7746e-02][val_loss:2.9307e-02][test_loss:2.9575e-02][teacher_ratio:0.0121] \n",
      "[Epoch:397][train_loss:8.6285e-03][val_loss:1.2787e-02][test_loss:2.9036e-02][teacher_ratio:0.0121] \n",
      "[Epoch:398][train_loss:9.3160e-03][val_loss:9.6655e-03][test_loss:3.4421e-02][teacher_ratio:0.0121] \n",
      "[Epoch:399][train_loss:1.0850e-02][val_loss:1.0124e-02][test_loss:3.2681e-02][teacher_ratio:0.0121] \n",
      "[Epoch:400][train_loss:8.0578e-03][val_loss:1.0548e-02][test_loss:3.0978e-02][teacher_ratio:0.0121] \n",
      "[Epoch:401][train_loss:1.0408e-02][val_loss:1.6317e-02][test_loss:2.6272e-02][teacher_ratio:0.0121] \n",
      "[Epoch:402][train_loss:8.7339e-03][val_loss:1.9121e-02][test_loss:2.6084e-02][teacher_ratio:0.0121] \n",
      "[Epoch:403][train_loss:5.0281e-03][val_loss:2.0405e-02][test_loss:2.6463e-02][teacher_ratio:0.0121] \n",
      "[Epoch:404][train_loss:1.0999e-02][val_loss:1.6657e-02][test_loss:2.6821e-02][teacher_ratio:0.0121] \n",
      "[Epoch:405][train_loss:3.9472e-03][val_loss:1.5719e-02][test_loss:2.6910e-02][teacher_ratio:0.0085] \n",
      "[Epoch:406][train_loss:6.8412e-03][val_loss:1.4170e-02][test_loss:2.6996e-02][teacher_ratio:0.0085] \n",
      "[Epoch:407][train_loss:5.0389e-03][val_loss:1.4208e-02][test_loss:2.6956e-02][teacher_ratio:0.0085] \n",
      "[Epoch:408][train_loss:4.5025e-03][val_loss:1.3497e-02][test_loss:2.6943e-02][teacher_ratio:0.0085] \n",
      "[Epoch:409][train_loss:5.4357e-03][val_loss:1.2630e-02][test_loss:2.7253e-02][teacher_ratio:0.0085] \n",
      "[Epoch:410][train_loss:5.9809e-03][val_loss:1.0229e-02][test_loss:2.8491e-02][teacher_ratio:0.0085] \n",
      "[Epoch:411][train_loss:4.9863e-03][val_loss:7.9227e-03][test_loss:3.0439e-02][teacher_ratio:0.0085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:412][train_loss:4.4474e-03][val_loss:7.9891e-03][test_loss:3.0018e-02][teacher_ratio:0.0085] \n",
      "[Epoch:413][train_loss:7.7056e-03][val_loss:1.2663e-02][test_loss:2.8036e-02][teacher_ratio:0.0085] \n",
      "[Epoch:414][train_loss:8.1163e-03][val_loss:1.8806e-02][test_loss:2.8058e-02][teacher_ratio:0.0085] \n",
      "[Epoch:415][train_loss:6.4930e-03][val_loss:2.2794e-02][test_loss:2.7577e-02][teacher_ratio:0.0085] \n",
      "[Epoch:416][train_loss:7.9354e-03][val_loss:2.0837e-02][test_loss:2.6722e-02][teacher_ratio:0.0085] \n",
      "[Epoch:417][train_loss:8.5068e-03][val_loss:1.0561e-02][test_loss:2.6869e-02][teacher_ratio:0.0085] \n",
      "[Epoch:418][train_loss:6.3036e-03][val_loss:9.4756e-03][test_loss:2.7247e-02][teacher_ratio:0.0085] \n",
      "[Epoch:419][train_loss:5.8047e-03][val_loss:1.3445e-02][test_loss:2.6448e-02][teacher_ratio:0.0085] \n",
      "[Epoch:420][train_loss:6.0544e-03][val_loss:2.1680e-02][test_loss:2.8239e-02][teacher_ratio:0.0085] \n",
      "[Epoch:421][train_loss:6.3926e-03][val_loss:2.2294e-02][test_loss:2.7563e-02][teacher_ratio:0.0085] \n",
      "[Epoch:422][train_loss:6.9948e-03][val_loss:2.2105e-02][test_loss:2.6338e-02][teacher_ratio:0.0085] \n",
      "[Epoch:423][train_loss:6.8885e-03][val_loss:1.3471e-02][test_loss:2.3919e-02][teacher_ratio:0.0085] \n",
      "[Epoch:424][train_loss:3.9771e-03][val_loss:1.1250e-02][test_loss:2.4360e-02][teacher_ratio:0.0085] \n",
      "[Epoch:425][train_loss:6.7074e-03][val_loss:1.4037e-02][test_loss:2.2718e-02][teacher_ratio:0.0085] \n",
      "[Epoch:426][train_loss:6.1548e-03][val_loss:2.5954e-02][test_loss:2.3965e-02][teacher_ratio:0.0085] \n",
      "[Epoch:427][train_loss:1.2537e-02][val_loss:1.7379e-02][test_loss:2.2301e-02][teacher_ratio:0.0085] \n",
      "[Epoch:428][train_loss:8.2086e-03][val_loss:9.2944e-03][test_loss:2.6112e-02][teacher_ratio:0.0085] \n",
      "[Epoch:429][train_loss:8.2427e-03][val_loss:8.0929e-03][test_loss:2.7602e-02][teacher_ratio:0.0085] \n",
      "[Epoch:430][train_loss:9.8718e-03][val_loss:1.5232e-02][test_loss:2.9182e-02][teacher_ratio:0.0085] \n",
      "[Epoch:431][train_loss:9.8559e-03][val_loss:3.1892e-02][test_loss:3.7984e-02][teacher_ratio:0.0085] \n",
      "[Epoch:432][train_loss:7.5990e-03][val_loss:3.0661e-02][test_loss:3.6841e-02][teacher_ratio:0.0085] \n",
      "[Epoch:433][train_loss:1.3245e-02][val_loss:9.8468e-03][test_loss:2.6196e-02][teacher_ratio:0.0085] \n",
      "[Epoch:434][train_loss:6.6356e-03][val_loss:8.1358e-03][test_loss:2.8449e-02][teacher_ratio:0.0085] \n",
      "[Epoch:435][train_loss:1.1296e-02][val_loss:1.2393e-02][test_loss:2.2299e-02][teacher_ratio:0.0085] \n",
      "[Epoch:436][train_loss:4.8293e-03][val_loss:2.2649e-02][test_loss:2.1008e-02][teacher_ratio:0.0085] \n",
      "[Epoch:437][train_loss:7.9398e-03][val_loss:2.1795e-02][test_loss:2.0949e-02][teacher_ratio:0.0085] \n",
      "[Epoch:438][train_loss:7.4027e-03][val_loss:1.4171e-02][test_loss:2.2043e-02][teacher_ratio:0.0085] \n",
      "[Epoch:439][train_loss:6.7067e-03][val_loss:9.1609e-03][test_loss:2.6577e-02][teacher_ratio:0.0085] \n",
      "[Epoch:440][train_loss:7.9650e-03][val_loss:9.9322e-03][test_loss:2.6466e-02][teacher_ratio:0.0085] \n",
      "[Epoch:441][train_loss:9.1833e-03][val_loss:2.5423e-02][test_loss:3.0362e-02][teacher_ratio:0.0085] \n",
      "[Epoch:442][train_loss:1.3209e-02][val_loss:2.8881e-02][test_loss:3.2788e-02][teacher_ratio:0.0085] \n",
      "[Epoch:443][train_loss:1.3773e-02][val_loss:1.4737e-02][test_loss:2.7847e-02][teacher_ratio:0.0085] \n",
      "[Epoch:444][train_loss:8.8193e-03][val_loss:9.1220e-03][test_loss:3.4869e-02][teacher_ratio:0.0085] \n",
      "[Epoch:445][train_loss:1.4562e-02][val_loss:1.0556e-02][test_loss:4.0000e-02][teacher_ratio:0.0085] \n",
      "[Epoch:446][train_loss:1.7820e-02][val_loss:8.7969e-03][test_loss:2.8061e-02][teacher_ratio:0.0085] \n",
      "[Epoch:447][train_loss:6.5459e-03][val_loss:2.6190e-02][test_loss:2.4331e-02][teacher_ratio:0.0085] \n",
      "[Epoch:448][train_loss:7.2917e-03][val_loss:4.1185e-02][test_loss:2.7253e-02][teacher_ratio:0.0085] \n",
      "[Epoch:449][train_loss:1.5149e-02][val_loss:2.0077e-02][test_loss:2.2748e-02][teacher_ratio:0.0085] \n",
      "[Epoch:450][train_loss:6.8866e-03][val_loss:7.6317e-03][test_loss:3.3168e-02][teacher_ratio:0.0085] \n",
      "[Epoch:451][train_loss:8.8860e-03][val_loss:1.0432e-02][test_loss:4.4654e-02][teacher_ratio:0.0085] \n",
      "[Epoch:452][train_loss:1.9684e-02][val_loss:7.0754e-03][test_loss:3.1808e-02][teacher_ratio:0.0085] \n",
      "[Epoch:453][train_loss:6.0583e-03][val_loss:1.5650e-02][test_loss:2.4771e-02][teacher_ratio:0.0085] \n",
      "[Epoch:454][train_loss:7.6260e-03][val_loss:2.6639e-02][test_loss:2.7440e-02][teacher_ratio:0.0085] \n",
      "[Epoch:455][train_loss:6.1857e-03][val_loss:2.6333e-02][test_loss:2.9328e-02][teacher_ratio:0.0059] \n",
      "[Epoch:456][train_loss:1.0455e-02][val_loss:1.5166e-02][test_loss:2.7871e-02][teacher_ratio:0.0059] \n",
      "[Epoch:457][train_loss:4.0234e-03][val_loss:8.8074e-03][test_loss:2.9874e-02][teacher_ratio:0.0059] \n",
      "[Epoch:458][train_loss:9.7544e-03][val_loss:6.7299e-03][test_loss:3.3915e-02][teacher_ratio:0.0059] \n",
      "[Epoch:459][train_loss:1.1615e-02][val_loss:6.7295e-03][test_loss:3.3811e-02][teacher_ratio:0.0059] \n",
      "[Epoch:460][train_loss:1.5898e-02][val_loss:1.2820e-02][test_loss:2.6545e-02][teacher_ratio:0.0059] \n",
      "[Epoch:461][train_loss:7.8046e-03][val_loss:2.6320e-02][test_loss:2.6283e-02][teacher_ratio:0.0059] \n",
      "[Epoch:462][train_loss:1.0739e-02][val_loss:2.4817e-02][test_loss:2.3495e-02][teacher_ratio:0.0059] \n",
      "[Epoch:463][train_loss:5.0394e-03][val_loss:1.8188e-02][test_loss:2.1182e-02][teacher_ratio:0.0059] \n",
      "[Epoch:464][train_loss:5.6941e-03][val_loss:1.2011e-02][test_loss:2.3975e-02][teacher_ratio:0.0059] \n",
      "[Epoch:465][train_loss:7.6262e-03][val_loss:1.0297e-02][test_loss:2.8193e-02][teacher_ratio:0.0059] \n",
      "[Epoch:466][train_loss:9.4860e-03][val_loss:1.0982e-02][test_loss:2.5059e-02][teacher_ratio:0.0059] \n",
      "[Epoch:467][train_loss:5.1692e-03][val_loss:1.6083e-02][test_loss:2.1005e-02][teacher_ratio:0.0059] \n",
      "[Epoch:468][train_loss:5.8895e-03][val_loss:2.3284e-02][test_loss:2.0553e-02][teacher_ratio:0.0059] \n",
      "[Epoch:469][train_loss:1.0630e-02][val_loss:1.6906e-02][test_loss:2.1650e-02][teacher_ratio:0.0059] \n",
      "[Epoch:470][train_loss:5.9468e-03][val_loss:8.7933e-03][test_loss:2.5646e-02][teacher_ratio:0.0059] \n",
      "[Epoch:471][train_loss:8.5304e-03][val_loss:7.5253e-03][test_loss:2.7665e-02][teacher_ratio:0.0059] \n",
      "[Epoch:472][train_loss:7.9490e-03][val_loss:8.7865e-03][test_loss:2.6264e-02][teacher_ratio:0.0059] \n",
      "[Epoch:473][train_loss:8.7688e-03][val_loss:2.0168e-02][test_loss:2.7416e-02][teacher_ratio:0.0059] \n",
      "[Epoch:474][train_loss:6.6852e-03][val_loss:2.7353e-02][test_loss:2.9777e-02][teacher_ratio:0.0059] \n",
      "[Epoch:475][train_loss:1.1904e-02][val_loss:1.5615e-02][test_loss:2.5505e-02][teacher_ratio:0.0059] \n",
      "[Epoch:476][train_loss:5.1539e-03][val_loss:8.3784e-03][test_loss:2.5754e-02][teacher_ratio:0.0059] \n",
      "[Epoch:477][train_loss:7.9177e-03][val_loss:7.3450e-03][test_loss:2.7667e-02][teacher_ratio:0.0059] \n",
      "[Epoch:478][train_loss:7.5115e-03][val_loss:1.0054e-02][test_loss:2.3297e-02][teacher_ratio:0.0059] \n",
      "[Epoch:479][train_loss:5.3995e-03][val_loss:2.4547e-02][test_loss:2.1037e-02][teacher_ratio:0.0059] \n",
      "[Epoch:480][train_loss:1.2586e-02][val_loss:3.2371e-02][test_loss:2.3195e-02][teacher_ratio:0.0059] \n",
      "[Epoch:481][train_loss:2.0319e-02][val_loss:1.1579e-02][test_loss:2.2744e-02][teacher_ratio:0.0059] \n",
      "[Epoch:482][train_loss:5.6339e-03][val_loss:8.5138e-03][test_loss:3.2023e-02][teacher_ratio:0.0059] \n",
      "[Epoch:483][train_loss:1.9518e-02][val_loss:8.5213e-03][test_loss:3.0535e-02][teacher_ratio:0.0059] \n",
      "[Epoch:484][train_loss:1.3865e-02][val_loss:1.2429e-02][test_loss:2.4679e-02][teacher_ratio:0.0059] \n",
      "[Epoch:485][train_loss:6.0219e-03][val_loss:3.0957e-02][test_loss:3.0650e-02][teacher_ratio:0.0059] \n",
      "[Epoch:486][train_loss:4.8236e-03][val_loss:4.6682e-02][test_loss:3.9393e-02][teacher_ratio:0.0059] \n",
      "[Epoch:487][train_loss:1.6645e-02][val_loss:3.6118e-02][test_loss:3.3114e-02][teacher_ratio:0.0059] \n",
      "[Epoch:488][train_loss:1.9089e-02][val_loss:1.6555e-02][test_loss:2.3502e-02][teacher_ratio:0.0059] \n",
      "[Epoch:489][train_loss:7.1022e-03][val_loss:1.0718e-02][test_loss:2.6390e-02][teacher_ratio:0.0059] \n",
      "[Epoch:490][train_loss:1.4151e-02][val_loss:1.1596e-02][test_loss:2.7453e-02][teacher_ratio:0.0059] \n",
      "[Epoch:491][train_loss:1.1588e-02][val_loss:1.4813e-02][test_loss:2.1416e-02][teacher_ratio:0.0059] \n",
      "[Epoch:492][train_loss:6.9241e-03][val_loss:2.6925e-02][test_loss:1.8786e-02][teacher_ratio:0.0059] \n",
      "[Epoch:493][train_loss:7.4131e-03][val_loss:3.6785e-02][test_loss:1.9984e-02][teacher_ratio:0.0059] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:494][train_loss:9.1278e-03][val_loss:3.0962e-02][test_loss:1.9090e-02][teacher_ratio:0.0059] \n",
      "[Epoch:495][train_loss:1.1522e-02][val_loss:1.5085e-02][test_loss:2.0826e-02][teacher_ratio:0.0059] \n",
      "[Epoch:496][train_loss:6.1658e-03][val_loss:1.0829e-02][test_loss:2.5042e-02][teacher_ratio:0.0059] \n",
      "[Epoch:497][train_loss:7.4075e-03][val_loss:9.2409e-03][test_loss:2.7978e-02][teacher_ratio:0.0059] \n",
      "[Epoch:498][train_loss:1.3466e-02][val_loss:1.1140e-02][test_loss:2.3893e-02][teacher_ratio:0.0059] \n",
      "[Epoch:499][train_loss:7.3999e-03][val_loss:2.8382e-02][test_loss:2.5973e-02][teacher_ratio:0.0059] \n",
      "[Epoch:500][train_loss:7.0552e-03][val_loss:4.3386e-02][test_loss:3.3394e-02][teacher_ratio:0.0059] \n"
     ]
    }
   ],
   "source": [
    "    process.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    process.analyse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    process.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
